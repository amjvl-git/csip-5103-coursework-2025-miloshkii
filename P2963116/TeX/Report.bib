@article{seaborn,
	doi = {10.21105/joss.03021},
	url = {https://doi.org/10.21105/joss.03021},
	year = {2021},
	publisher = {The Open Journal},
	volume = {6},
	number = {60},
	pages = {3021},
	author = {Michael L. Waskom},
	title = {seaborn: statistical data visualization},
	journal = {Journal of Open Source Software}
}
@Article{matplotlib,
	Author    = {Hunter, J. D.},
	Title     = {Matplotlib: A 2D graphics environment},
	Journal   = {Computing in Science \& Engineering},
	Volume    = {9},
	Number    = {3},
	Pages     = {90--95},
	abstract  = {Matplotlib is a 2D graphics package used for Python for
	application development, interactive scripting, and publication-quality
	image generation across user interfaces and operating systems.},
	publisher = {IEEE COMPUTER SOC},
	doi       = {10.1109/MCSE.2007.55},
	year      = 2007
}
@misc{pytorch,
	title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
	author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
	year={2019},
	eprint={1912.01703},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1912.01703}, 
}
@online{mnist,
	author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
	title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	date         = {2017-08-28},
	year         = {2017},
	eprintclass  = {cs.LG},
	eprinttype   = {arXiv},
	eprint       = {cs.LG/1708.07747},
}
@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}
@article{dropout,
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	year = {2014},
	issue_date = {January 2014},
	publisher = {JMLR.org},
	volume = {15},
	number = {1},
	issn = {1532-4435},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	journal = {J. Mach. Learn. Res.},
	month = jan,
	pages = {1929–1958},
	numpages = {30},
	keywords = {deep learning, model combination, neural networks, regularization}
}
@misc{batchnorm,
	title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
	author={Sergey Ioffe and Christian Szegedy},
	year={2015},
	eprint={1502.03167},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1502.03167}, 
}
@InProceedings{pandas,
	author    = { {W}es {M}c{K}inney },
	title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
	booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
	pages     = { 56 - 61 },
	year      = { 2010 },
	editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
	doi       = { 10.25080/Majora-92bf1922-00a }
}
@online{nnsvg,
	author = {Alex Lenail},
	title = {NN SVG},
	date = {2025-10-25},
	url = {https://alexlenail.me/NN-SVG/AlexNet.html},
	year = {unknown},
	urldate = {2025-10-25},
}

@online{CentraleSupelec,
	author = {Jeremy Fix},
	title = {First steps in PyTorch},
	url = {https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html},
	subtitle = { Classifying fashion objects (FashionMNIST)},
	organization = {CentraleSupelec},

}

@online{featureref,
	author = {Subhajeet Das},
	title = {PyTorch Feature Map and Filters Visualization},
	date = {2024},
	url = {https://www.kaggle.com/code/subhajeetdas/pytorch-feature-map-filters-visualization},
}

@online{overleaf,
	author = {Overleaf},
	title = {Learn LaTeX},
	date = {2025-11-01},
	url = {https://www.overleaf.com/learn},
}
