{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15369fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu130\n",
      "PyTorch working with CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('PyTorch working with CUDA:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73855e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordered list of data labels as per Fashion MNIST Documentation\n",
    "data_labels = [\"T-Shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\",\n",
    "               \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
    "\n",
    "# Data pre-processing transformations\n",
    "transforms = Compose([\n",
    "    ToTensor(), \n",
    "    Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "# Initialises data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms\n",
    ")\n",
    "\n",
    "# Batch Size Hyperparameter (default = 64)\n",
    "batch_size = 64\n",
    "\n",
    "# Load the data with automatic shuffling for training\n",
    "train_loader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "# Tests\n",
    "train_expected_size = torch.Size([batch_size,1,28,28])\n",
    "assert train_images.shape == train_expected_size\n",
    "\n",
    "expected_label_size = torch.Size([batch_size])\n",
    "assert train_labels.shape == expected_label_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e10bd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of random sample with label\n",
    "img = train_images[0].squeeze()\n",
    "label = data_labels[train_labels[0]]\n",
    "\n",
    "fig = plt.imshow(img)\n",
    "plt.axis('off')\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "plt.set_cmap('inferno')\n",
    "plt.title(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52665704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, hidden1,hidden2,hidden3,in_features=28*28, out_features=10):\n",
    "        '''\n",
    "        3-Layer MLP with specifiable hidden layer size\n",
    "        Args:\n",
    "            hidden1 (int): specify the number of neurons in layer 1\n",
    "            hidden2 (int): specify the number of neurons in layer 2\n",
    "            hidden3 (int): specify the number of neurons in layer 3\n",
    "            in_features (int/optional): specify the number of inputs (size of flattened image = length * width)\n",
    "            out_features (int/optional): specify the number of outputs (number of classes)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features,hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2,hidden3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden3, out_features),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Perform a forward pass of the network.\n",
    "        Note: This flattens the image for us so we need not reshape input data.\n",
    "        Args:\n",
    "            x (tensor): datapoint from our data\n",
    "        '''\n",
    "        x = self.flatten(x)\n",
    "        output = self.lin_relu_stack(x)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c608c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (lin_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise model and check structure\n",
    "ann_model = MultilayerPerceptron(512,256,128)\n",
    "print(ann_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "781bf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optim):\n",
    "    '''\n",
    "    Perform a simple training pass for one epoch\n",
    "    Args:\n",
    "        dataloader (Dataloader(data)): Dataloader using torchvision\n",
    "        model (ANN/CNN): Chosen model\n",
    "        loss_fn: Loss function used\n",
    "        optim: optimiser used for gradient parameter adjustment\n",
    "    '''\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batch = len(dataloader)\n",
    "    train_loss = 0\n",
    "    for batch, (X,label) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss +=loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= num_batch\n",
    "    print(f'Training loss = {train_loss:>5f}')\n",
    "    return train_loss\n",
    "\n",
    "def test(dataloader, model):\n",
    "    '''\n",
    "    Perform a test pass.\n",
    "    Args:\n",
    "        dataloader (DataLoader(data)): Dataloader of a dataset using torchvision\n",
    "        model (ANN/CNN): Chosen model\n",
    "    '''\n",
    "    model.eval()\n",
    "    num_batch = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69f36a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial hyperparameters\n",
    "learn_rate = 1e-3\n",
    "epochs = 2\n",
    "\n",
    "# initialise loss function and optimiser\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ann_model.parameters(), lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bc3ecc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "loss: 1.650502  [   64/60000]\n",
      "loss: 1.633060  [ 6464/60000]\n",
      "loss: 1.586220  [12864/60000]\n",
      "loss: 1.679853  [19264/60000]\n",
      "loss: 1.650713  [25664/60000]\n",
      "loss: 1.652387  [32064/60000]\n",
      "loss: 1.638659  [38464/60000]\n",
      "loss: 1.525944  [44864/60000]\n",
      "loss: 1.601668  [51264/60000]\n",
      "loss: 1.745731  [57664/60000]\n",
      "Training loss = 1.646344\n",
      "Epoch 2/2\n",
      "loss: 1.725412  [   64/60000]\n",
      "loss: 1.633025  [ 6464/60000]\n",
      "loss: 1.586151  [12864/60000]\n",
      "loss: 1.595748  [19264/60000]\n",
      "loss: 1.601627  [25664/60000]\n",
      "loss: 1.632838  [32064/60000]\n",
      "loss: 1.633017  [38464/60000]\n",
      "loss: 1.617401  [44864/60000]\n",
      "loss: 1.692916  [51264/60000]\n",
      "loss: 1.586149  [57664/60000]\n",
      "Training loss = 1.640421\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m     train(train_loader, ann_model, loss_fn, optimizer)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m#loss_hist.append(train_loss)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_loss\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "for e in range(epochs):\n",
    "    print(f'Epoch {e+1}/{epochs}')\n",
    "    train(train_loader, ann_model, loss_fn, optimizer)\n",
    "    #loss_hist.append(train_loss)\n",
    "print(train_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
